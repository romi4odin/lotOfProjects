{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. what is supervised learning\n",
    "2. what is scatter plot\n",
    "3. what are the different types of plot we use\n",
    "4. what is the difference between bar plot and histogram\n",
    "5. what are the different types of evaluation for classification model\n",
    "6. what is recall, precision\n",
    "7. How to know the duplicate rows and remove them\n",
    "8. what is scatter plot\n",
    "9. any interactive plot\n",
    "10. any cloud services known\n",
    "11. how would you know the number of rows and columns\n",
    "12. hold on scatter plot\n",
    "13. remove duplicate rows in dataframe\n",
    "14. steps of data modelling or machine leanring model\n",
    "15. what is PCA \n",
    "16. scenario based question : how would you know the number of petrol stations in your city\n",
    "17. What is left join\n",
    "18. SQL query for avg of salary from emp table in various departments\n",
    "19. What are the libraries used for plotting\n",
    "20. Have you done any interactive graph , where we can zoom in zoom out click on something\n",
    "21. Explain the projects\n",
    "22. How do you deal with missing values\n",
    "23. What is multi-collinearity\n",
    "24. Lifecycle of Data model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When we have labeled dataset we use supervised learning and when we don't have the labeled data and we are particularly confused about the underlying patterns of the data . From the data it discovers the clustering or association .\n",
    "Common Examples are : Hierarchical , k-means, gaussian mixture model\n",
    "\n",
    "2. Scatter plots are used to observe relationships between variables.uses dots to represent values for two different numeric variables\n",
    "\n",
    "3. scatter plot , bar plot , line plot , box plot , pie chart , bubble chart , funnel plot, violin plots , dual axis chart\n",
    "\n",
    "4. A bar graph is used to compare discrete or categorical data but the histogram depicts the frequency distribution of variables in dataset \n",
    "\n",
    "5. Confusion Matrix (Accuracy, Precision , recall , F1 score), ROC AUC curve , log loss (cross entropy loss)\n",
    "\n",
    "Confusion Matrix is the tabular representaion of the predicted values and Actual values in a classification model\n",
    "\n",
    "Actual/Predicted         False                              True\n",
    "Pregnant               False Negative (Type 2 Error)    True Positive\n",
    "Not Pregnant           True Negative                False Positive (Type 1 Error)\n",
    "\n",
    "Accuracy is a metric which shows that out of all the predicted , how many are correctly predicted that is\n",
    "\n",
    "Accuracy = TP + TN / TP + TN + FP + FN\n",
    "\n",
    "Precision is a metric which says that out of all positively predicted how many of them are actually positive. This is useful when we want to see which were  predicted positive even though they were negative. That is\n",
    "\n",
    "Precision = TP / TP + FP , Here we are caring about the False Positive (Type 1 Error)\n",
    "\n",
    "Recall is a metric which cares about the Type 2 error and is highly crucial in medical domain . It says that out of all negatively classified how many of them are actually negative..\n",
    "\n",
    "Recall = TN / FN + TN\n",
    "\n",
    "F1 Score = Harmonic mean of Precision and recall, it punishes the extreme values\n",
    "\n",
    "ROC (Reciever operator characteristics ) : It is the plot between true positive rate to the false positive rate  at different thresholds and separates the signal from the noise\n",
    "\n",
    "AUC (Area under the curve) : It is the measure of a classifier to distinguish between the positive and negative clsses. If the area under the curve is 1 that means the classifier is perfectly able to classify. If the area under the curve is 0 then the classifier is classifying all the positive classes to negative and the negative classes to positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.PCA (Principal component analysis) : It is one of the feature reduction techniques. There are in general two feature reduction techniques. One is Feature Selection and the other one is Feature Extraction . PCA is the part of Feature Extraction. It is the one which captures the maximum variance of the feature columns\n",
    "\n",
    "Steps to calculate the Eigen Vector :\n",
    "Step 1 : Calculate the mean of each feature columns\n",
    "Step 2 : Calculate the covariance matrix , summation of all the square of x-xmean / n , where n is the number of data points\n",
    "Step 3 : Calculate the Eigen values , Covariance matrix - lambda * Identity Matrix\n",
    "Step 4 : calculate the Eigen vectors ,( Covariance matrix - lambda * Identity Matrix ) * Eigen vector\n",
    "Step 5 : Transfor the feature vector , Transpose of Eigen vector * (Feature vector - Maan vector)\n",
    "\n",
    "\n",
    "LDA (Linear Discriminant Analysis) : It is also a Feature Extraction technique. This is a supervised technique that is  it takes the feature labels into account. It is used to find a feature space that has the best separabilty for the features so that they can be easily separable. Here Scatter matrix is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso ( L1 Regularisation ) : Shrinks some of the coefficients to 0 , that is helpful in feature selection.\n",
    "\n",
    "extra term added is summation ( lambda * coefficients of the features )\n",
    "\n",
    "Ridge ( L2 Regularisation ) : Reduces the coefficients but never to the 0 . Helpful in the scenario where many features are correlated.\n",
    "\n",
    "extra term added is summation ( lambda * square of coefficients of the features )\n",
    "\n",
    "ElasticNet : combines both L1 and L2 regularisation . Helpful in the scenario where you want to handle multi-coliniearity as well as Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Types of cross validation :\n",
    "\n",
    "1. Holdout method\n",
    "2. K-fold cross validation\n",
    "3. Leave out one cross validation\n",
    "4. Leave out p cross validation\n",
    "5. Time series cross validation\n",
    "6. Stratified K-fold cross validation\n",
    "7. Group K-Fold cross validation\n",
    "8. Nested cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Different types of kernel tricks in SVM\n",
    "1. Ploynomial kernel\n",
    "2. Sigmoid kernel\n",
    "3. RBF kernel\n",
    "4. Anova kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction in variance is taken into consideration for regression problems in decision tree. \n",
    "For classification problem we use gini index, entropy , information gain , chi square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble techniques :\n",
    "\n",
    "1. Bagging\n",
    "    Random Forest classifier\n",
    "    Random Forest Regressor\n",
    "2. Boosting\n",
    "    Adaboost\n",
    "    XGBoost\n",
    "    Gradient boosting Machines\n",
    "    LightGBM\n",
    "3. Stacking\n",
    "    It combines the strong learners\n",
    "    It combines heterogeneous models\n",
    "    It consists of creating a MetaModel. A MetaModel is one which learns how to combine the result of various models. The output of various models are fed as input to the MetaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Lifecycle of Model \n",
    "\n",
    "1. Problem definition\n",
    "2. Data collection :\n",
    "     Identify the sources of information , APIs or web scraping, databases\n",
    "     Make sure the data is relevant\n",
    "\n",
    "3. Data preparation : (clean the data)\n",
    "    Handle missing values\n",
    "    Remove duplicates\n",
    "    Address outliers\n",
    "    Normalize and standardize the data\n",
    "    Encode the categorical data\n",
    "    Feture Engineering(Creating new features , transforming the existing ones)\n",
    "    split the data into training and testing\n",
    "\n",
    "\n",
    "4. Exploratory Data Analysis : understand the data and uncover patterns , trends and relationships\n",
    "    visualize the data using charts , plots and graphs\n",
    "    calculate summary statistics\n",
    "    Identify correlations and actions between features\n",
    "    validate assumptions and hypothesis\n",
    "\n",
    "5. Feature Selection \n",
    "    Select the features based on goals, there can be filters and wrappers\n",
    "    Filters is something related to the statistical methods that can be correlation distances etc.\n",
    "    Wrappers are something based on the machine learning thing , it is an iterative process\n",
    "\n",
    "6. Feature Extraction \n",
    "    LDA (Linear discriminant Analysis)\n",
    "    PCA(Principal Component analysis)\n",
    "\n",
    "7. Model Selection\n",
    "\n",
    "8. Model Evaluation\n",
    "\n",
    "    Caluclate performance metrics\n",
    "    use confusion matrix\n",
    "\n",
    "9. Model deployment\n",
    "\n",
    "        REST\n",
    "\n",
    "10. Model monitoring and performance\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
